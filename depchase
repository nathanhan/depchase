#!/usr/bin/python3
import configparser
import itertools
import logging
import os
import sys
import tempfile
import click
import smartcols
import solv
import functools
import timeit
import modulemd
import pydot

XDG_CACHE_HOME = os.environ.get("XDG_CACHE_HOME") or os.path.expanduser("~/.cache")
CACHEDIR = os.path.join(XDG_CACHE_HOME, "depchase")

logger = logging.getLogger("depchase")

class Repo(object):
    def __init__(self, name, baseurl):
        self.name = name
        self.baseurl = baseurl
        self.handle = None
        self.cookie = None
        self.extcookie = None
        self.srcrepo = None

    @staticmethod
    def calc_cookie_fp(fp):
        chksum = solv.Chksum(solv.REPOKEY_TYPE_SHA256)
        chksum.add("1.1")
        chksum.add_fp(fp)
        return chksum.raw()

    @staticmethod
    def calc_cookie_ext(f, cookie):
        chksum = solv.Chksum(solv.REPOKEY_TYPE_SHA256)
        chksum.add("1.1")
        chksum.add(cookie)
        chksum.add_fstat(f.fileno())
        return chksum.raw()

    def cachepath(self, ext=None):
        path = self.name.replace(".", "_")
        if ext:
            path = "{}-{}.solvx".format(path, ext)
        else:
            path = "{}.solv".format(path)
        return os.path.join(CACHEDIR, path.replace("/", "_"))

    def usecachedrepo(self, ext, mark=False):
        try:
            repopath = self.cachepath(ext)
            f = open(repopath, "rb")
            f.seek(-32, os.SEEK_END)
            fcookie = f.read(32)
            if len(fcookie) != 32:
                return False
            cookie = self.extcookie if ext else self.cookie
            if cookie and fcookie != cookie:
                return False
            if not ext:
                f.seek(-32 * 2, os.SEEK_END)
                fextcookie = f.read(32)
                if len(fextcookie) != 32:
                    return False
            f.seek(0)
            f = solv.xfopen_fd(None, f.fileno())
            flags = 0
            if ext:
                flags = solv.Repo.REPO_USE_LOADING | solv.Repo.REPO_EXTEND_SOLVABLES
                if ext != "DL":
                    flags |= solv.Repo.REPO_LOCALPOOL
            if not self.handle.add_solv(f, flags):
                return False
            if not ext:
                self.cookie = fcookie
                self.extcookie = fextcookie
            if mark:
                # no futimes in python?
                try:
                    os.utime(repopath, None)
                except Exception:
                    pass
        except IOError:
            return False
        return True

    def writecachedrepo(self, ext, repodata=None):
        tmpname = None
        try:
            if not os.path.isdir(CACHEDIR):
                os.mkdir(CACHEDIR, 0o755)
            fd, tmpname = tempfile.mkstemp(prefix=".newsolv-", dir=CACHEDIR)
            os.fchmod(fd, 0o444)
            f = os.fdopen(fd, "wb+")
            f = solv.xfopen_fd(None, f.fileno())
            if not repodata:
                self.handle.write(f)
            elif ext:
                repodata.write(f)
            else:
                # rewrite_repos case, do not write stubs
                self.handle.write_first_repodata(f)
            f.flush()
            if not ext:
                if not self.extcookie:
                    self.extcookie = self.calc_cookie_ext(f, self.cookie)
                f.write(self.extcookie)
            if not ext:
                f.write(self.cookie)
            else:
                f.write(self.extcookie)
            f.close
            if self.handle.iscontiguous():
                # switch to saved repo to activate paging and save memory
                nf = solv.xfopen(tmpname)
                if not ext:
                    # main repo
                    self.handle.empty()
                    flags = solv.Repo.SOLV_ADD_NO_STUBS
                    if repodata:
                        # rewrite repos case, recreate stubs
                        flags = 0
                    if not self.handle.add_solv(nf, flags):
                        sys.exit("internal error, cannot reload solv file")
                else:
                    # extension repodata
                    # need to extend to repo boundaries, as this is how
                    # repodata.write() has written the data
                    repodata.extend_to_repo()
                    flags = solv.Repo.REPO_EXTEND_SOLVABLES
                    if ext != "DL":
                        flags |= solv.Repo.REPO_LOCALPOOL
                    repodata.add_solv(nf, flags)
            os.rename(tmpname, self.cachepath(ext))
        except (OSError, IOError):
            if tmpname:
                os.unlink(tmpname)

    def load(self, pool):
        assert not self.handle
        self.handle = pool.add_repo(self.name)
        self.handle.appdata = self
        f = self.download("repodata/repomd.xml", False, None)
        if not f:
            self.handle.free(True)
            self.handle = None
            return False
        self.cookie = self.calc_cookie_fp(f)
        if self.usecachedrepo(None, True):
            return True
        self.handle.add_repomdxml(f)
        fname, fchksum = self.find("primary")
        if not fname:
            return False
        f = self.download(fname, True, fchksum)
        if not f:
            return False
        self.handle.add_rpmmd(f, None)
        self.add_exts()
        self.writecachedrepo(None)
        # Must be called after writing the repo
        self.handle.create_stubs()
        return True

    def download(self, fname, uncompress, chksum):
        f = open("{}/{}".format(self.baseurl, fname))
        return solv.xfopen_fd(fname if uncompress else None, f.fileno())

    def find(self, what):
        di = self.handle.Dataiterator_meta(solv.REPOSITORY_REPOMD_TYPE, what, solv.Dataiterator.SEARCH_STRING)
        di.prepend_keyname(solv.REPOSITORY_REPOMD)
        for d in di:
            dp = d.parentpos()
            filename = dp.lookup_str(solv.REPOSITORY_REPOMD_LOCATION)
            chksum = dp.lookup_checksum(solv.REPOSITORY_REPOMD_CHECKSUM)
            if filename:
                if not chksum:
                    print("No {} file checksum!".format(filename))
                return filename, chksum
        return None, None

    def add_ext_keys(self, ext, repodata, handle):
        if ext == "FL":
            repodata.add_idarray(handle, solv.REPOSITORY_KEYS, solv.SOLVABLE_FILELIST)
            repodata.add_idarray(handle, solv.REPOSITORY_KEYS, solv.REPOKEY_TYPE_DIRSTRARRAY)
        else:
            raise NotImplementedError

    def add_ext(self, repodata, what, ext):
        filename, chksum = self.find(what)
        if not filename:
            return
        handle = repodata.new_handle()
        repodata.set_poolstr(handle, solv.REPOSITORY_REPOMD_TYPE, what)
        repodata.set_str(handle, solv.REPOSITORY_REPOMD_LOCATION, filename)
        repodata.set_checksum(handle, solv.REPOSITORY_REPOMD_CHECKSUM, chksum)
        self.add_ext_keys(ext, repodata, handle)
        repodata.add_flexarray(solv.SOLVID_META, solv.REPOSITORY_EXTERNAL, handle)

    def add_exts(self):
        repodata = self.handle.add_repodata()
        self.add_ext(repodata, "filelists", "FL")
        repodata.internalize()

    def load_ext(self, repodata):
        repomdtype = repodata.lookup_str(solv.SOLVID_META, solv.REPOSITORY_REPOMD_TYPE)
        if repomdtype == "filelists":
            ext = "FL"
        else:
            assert False
        if self.usecachedrepo(ext):
            return True
        filename = repodata.lookup_str(solv.SOLVID_META, solv.REPOSITORY_REPOMD_LOCATION)
        filechksum = repodata.lookup_checksum(solv.SOLVID_META, solv.REPOSITORY_REPOMD_CHECKSUM)
        f = self.download(filename, True, filechksum)
        if not f:
            return False
        if ext == "FL":
            self.handle.add_rpmmd(f, "FL", solv.Repo.REPO_USE_LOADING | solv.Repo.REPO_EXTEND_SOLVABLES | solv.Repo.REPO_LOCALPOOL)
        self.writecachedrepo(ext, repodata)
        return True

    def updateaddedprovides(self, addedprovides):
        if self.handle.isempty():
            return
        # make sure there's just one real repodata with extensions
        repodata = self.handle.first_repodata()
        if not repodata:
            return
        oldaddedprovides = repodata.lookup_idarray(solv.SOLVID_META, solv.REPOSITORY_ADDEDFILEPROVIDES)
        if not set(addedprovides) <= set(oldaddedprovides):
            for id in addedprovides:
                repodata.add_idarray(solv.SOLVID_META, solv.REPOSITORY_ADDEDFILEPROVIDES, id)
            repodata.internalize()
            self.writecachedrepo(None, repodata)

def load_stub(repodata):
    repo = repodata.repo.appdata
    if repo:
        return repo.load_ext(repodata)
    return False

def setup_repos(conffile):
    conf = configparser.ConfigParser(interpolation=configparser.ExtendedInterpolation())

    with open(conffile, "r") as cfg:
        conf.read_file(cfg)

    repos = {}
    for sect in conf.sections():
        repos[sect] = Repo(sect, conf[sect]["path"])
    for repo in repos.values():
        repo.srcrepo = repos.get("{}-source".format(repo.name))
    return list(repos.values())

def setup_pool(arch, repos=()):
    pool = solv.Pool()
    #pool.set_debuglevel(2)
    pool.setarch(arch)
    pool.set_loadcallback(load_stub)

    for repo in repos:
        repo.baseurl = repo.baseurl.format(arch=arch)

    start = timeit.default_timer()
    for repo in repos:
        assert repo.load(pool)
        if repo.name.endswith("-override"):
            repo.handle.priority = 99

    addedprovides = pool.addfileprovides_queue()
    if addedprovides:
        for repo in repos:
            repo.updateaddedprovides(addedprovides)

    pool.createwhatprovides()
    end = timeit.default_timer()
    logger.debug("Load repos: {}s".format(end - start))

    return pool

def fix_deps(pool):
    to_fix = (
        # Weak libcrypt-nss deps due to https://github.com/openSUSE/libsolv/issues/205
        ("glibc", solv.Selection.SELECTION_NAME,
         solv.SOLVABLE_RECOMMENDS, lambda s: s.startswith("libcrypt-nss"), solv.SOLVABLE_SUGGESTS),
        # Shim is not buildable
        ("shim", solv.Selection.SELECTION_NAME | solv.Selection.SELECTION_WITH_SOURCE,
         solv.SOLVABLE_REQUIRES, lambda s: s in ("gnu-efi = 3.0w", "gnu-efi-devel = 3.0w"), None),
        # Rust has useless dependency on rust-rpm-macro -> python3-rust2rpm -> python3-tqdm -> python3-matplotlib -> ...
        ("rust", solv.Selection.SELECTION_NAME,
         solv.SOLVABLE_REQUIRES, lambda s: s == "rust-rpm-macro", None),
    )
    for txt, flags, before, func, after in to_fix:
        for s in pool.select(txt, flags).solvables():
            deps = s.lookup_deparray(before)
            fixing = [dep for dep in deps if func(str(dep))]
            for dep in fixing:
                deps.remove(dep)
                if after is not None:
                    s.add_deparray(after, dep)
            # Use s.set_deparray() once will be available
            s.unset(before)
            for dep in deps:
                s.add_deparray(before, dep)

def get_sourcepkg(p, s=None, only_name=False):
    if s is None:
        s = p.lookup_sourcepkg()[:-4]
    if only_name:
        return s
    # Let's try to find corresponding source
    sel = p.pool.select(s, solv.Selection.SELECTION_CANON | solv.Selection.SELECTION_SOURCE_ONLY)
    sel.filter(p.repo.appdata.srcrepo.handle.Selection())
    assert not sel.isempty(), "Could not find source package for {}".format(s)
    solvables = sel.solvables()
    assert len(solvables) == 1
    return solvables[0]

def alternatives_table(solver):
    tb = smartcols.Table()
    tb.title = "Alternatives"
    tb.colors = True
    #tb.colors = os.isatty(sys.stdout.fileno())
    cl_num = tb.new_column("ID")
    cl_num.right = True
    cl_info = tb.new_column("INFO")
    for alt in solver.all_alternatives():
        if alt.type == solv.Alternative.SOLVER_ALTERNATIVE_TYPE_RECOMMENDS:
            raise NotImplementedError
        elif alt.type == solv.Alternative.SOLVER_ALTERNATIVE_TYPE_RULE:
            for rinfo in alt.rule.allinfos():
                if rinfo.type == solv.Solver.SOLVER_RULE_JOB:
                    raise NotImplementedError
                elif rinfo.type == solv.Solver.SOLVER_RULE_PKG_REQUIRES:
                    s = "{} requires {}".format(rinfo.solvable, rinfo.dep)
                else:
                    raise NotImplementedError
        else:
            assert False
        if [l for l in tb.lines() if str(l[cl_info].data) == s]:
            return tb
        ln = tb.new_line()
        ln[cl_num] = "0"
        ln[cl_info] = s
        num = 1
        raw_choices = alt.choices_raw()
        for choice in alt.choices():
            ln = tb.new_line()
            ln[cl_num] = str(num)
            if choice == alt.chosen:
                ln[cl_info].color = "green"
                ln[cl_info] = "+ {}".format(choice)
            elif -choice.id in raw_choices:
                # Auto-minimized
                ln[cl_info].color = "blue"
                ln[cl_info] = "- {}".format(choice)
            else:
                ln[cl_info] = "  {}".format(choice)
            num += 1

    return tb

def rules_table(solver, pkgs=None):
    if pkgs is None:
        pkgs = [solv.XSolvable(solver.pool, i) for i in solver.raw_decisions(1)]

    tb = smartcols.Table()
    cl = tb.new_column("INFO")
    cl.tree = True
    for p in pkgs:
        reason, rule = solver.describe_decision(p)
        if reason == solv.Solver.SOLVER_REASON_UNRELATED:
            continue

        if reason == solv.Solver.SOLVER_REASON_WEAKDEP:
            raise NotImplementedError

        ln = tb.new_line()
        ln[cl] = str(p)
        if rule is not None:
            for info in rule.allinfos():
                if info.type == solv.Solver.SOLVER_RULE_JOB:
                    s = "requested by user"
                elif info.type == solv.Solver.SOLVER_RULE_PKG_REQUIRES:
                    s = "{} requires {}".format(info.solvable, info.dep)
                else:
                    print(info.type)
                    raise NotImplementedError
                lnr = tb.new_line(ln)
                lnr[cl] = s
        else:
            assert False

    return tb

def solve(solver, pkgnames, selfhost=False):
    pool = solver.pool

    # We have to =(
    start = timeit.default_timer()
    fix_deps(pool)
    end = timeit.default_timer()
    logger.debug("\"Fixing\" deps: {}s".format(end - start))

    start = timeit.default_timer()
    jobs = []
    # Initial jobs, no conflicting packages
    for n in pkgnames:
        sel = pool.select(n, solv.Selection.SELECTION_NAME | solv.Selection.SELECTION_DOTARCH)
        assert not sel.isempty(), "Could not find package for {}".format(n)
        jobs += sel.jobs(solv.Job.SOLVER_INSTALL)
    problems = solver.solve(jobs)
    if problems:
        for p in problems:
            print(p)
        sys.exit(1)
    end = timeit.default_timer()
    logger.debug("Runtime depsolve: {}s".format(end - start))

    candq = solver.transaction().newpackages()
    sources = set(get_sourcepkg(s) for s in candq)

    logger.debug(alternatives_table(solver))
    logger.debug(rules_table(solver))

    if not selfhost:
        return set(candq), sources

    #solver.write_testcase("/home/brain/tmp-debug")

    start = timeit.default_timer()
    # We already solved runtime requires, no need to do that twice
    selfhosting = set(candq)
    selfhosting_srcs = set()
    candq = list(sources)
    # We will store text-based view of processed srcs for better performance,
    # because selections are not free
    srcs_done = set()
    while candq:
        jobs = [pool.Job(solv.Job.SOLVER_INSTALL | solv.Job.SOLVER_SOLVABLE | solv.Job.SOLVER_WEAK, p.id) for p in candq]
        solver.solve(jobs)
        # We are interested to operate only on really new packages below
        newpkgs = set(solver.transaction().newpackages()) - selfhosting
        for p in newpkgs.copy():
            if p.arch in ("src", "nosrc"):
                srcs_done.add(str(p))
                selfhosting_srcs.add(p)
                newpkgs.remove(p)
                continue
            #if p.name in ("lato-fonts",):
            #    reason, rule = solver.describe_decision(p)
            #    for info in rule.allinfos():
            #        if info.type == solv.Solver.SOLVER_RULE_JOB:
            #            # Not interested in user requests
            #            continue
            #        elif info.type == solv.Solver.SOLVER_RULE_PKG_REQUIRES:
            #            print("{} ← {} requires {}".format(p, info.solvable, info.dep))
            #        else:
            #            assert False
        selfhosting |= newpkgs
        logger.debug(alternatives_table(solver))
        logger.debug(rules_table(solver))

        # SOLVER_FAVOR packages which we already solved which will help us to get small dependency chain
        pool.setpooljobs(pool.getpooljobs() + [pool.Job(solv.Job.SOLVER_FAVOR | solv.Job.SOLVER_SOLVABLE, p.id) for p in newpkgs])

        #print(alternatives_table(solver))
        # In new queue only non-solvables are left
        raw_decisions = solver.raw_decisions(1)
        if not raw_decisions:
            # At this point, nothing can be resolved anymore, so let's show problems
            for p in candq:
                job = pool.Job(solv.Job.SOLVER_INSTALL | solv.Job.SOLVER_SOLVABLE, p.id)
                problems = solver.solve([job])
                # In some cases, even solvable jobs are disabled
                # https://github.com/openSUSE/libsolv/issues/204
                #assert not problems
                for problem in problems:
                    print(problem)
            sys.exit(1)
        candq = [s for s in candq if s.id not in raw_decisions]

        srcs_queued = set(str(p) for p in candq if p.arch in ("src", "nosrc"))
        for p in newpkgs:
            s = get_sourcepkg(p, only_name=True)
            if s in srcs_done or s in srcs_queued:
                continue
            src = get_sourcepkg(p, s)
            srcs_queued.add(str(src))
            candq.append(src)
    end = timeit.default_timer()
    logger.debug("Self-hosting depsolve: {}s".format(end - start))

    return selfhosting, selfhosting_srcs

##functions for grapher below
#parse nevra
def simplifypackname(name):
    woarch = name.rsplit('.', 1)[0]
    worel = woarch.rsplit('-', 1)[0]
    wover = worel.rsplit('-', 1)[0]

    return wover

#store depchase rules table output in dict, returns dictionary with runtime dependency packages as keys and rationale as values
def chasedeps2(rawresults):
    depinfo = rawresults.split("\n")
    if "INFO" in depinfo:
        depinfo2 = depinfo[depinfo.index("INFO")+1:]
    else:
        print("ERROR! Package name not found")
    finaldependencyinfo = {}
    index = 0
    while len(depinfo2) > 2 :
        if depinfo2[index][1] == '─' and depinfo2[index+1][1] != '─':
            key = depinfo2[0]
            finaldependencyinfo[simplifypackname(key)] = list(set([simplifypackname(i) for i in [i.split(" requires")[0][2:] for i in depinfo2[1:index+1]]]))
            del depinfo2[:index+1]
            index=0
        else:
            index+=1
    finaldependencyinfo[simplifypackname(depinfo2[0])] = list(set([simplifypackname(i) for i in [i.split(" requires")[0][2:] for i in depinfo2[1:]]]))
    
    return finaldependencyinfo

#open modulemd io once and take care of business to avoid io bottleneck
def onetimeload(inframodules):
    dictionary = {}
    modfile = modulemd.ModuleMetadata()
    for location in inframodules:
        modfile.load(location)
        inframodulename = os.path.basename(location).split(".yaml")[0]
        dictionary[inframodulename] = modfile.api.rpms

    return dictionary

#parse grapher input file
def readgraphmakerinput(file):
    with open(file) as inputfile:
        content = [x.strip() for x in inputfile.readlines()]
        content = [x for x in content if x]
        big3 = content[content.index("existing_modules_start")+1:content.index("existing_modules_end")]
        custom = []
        ignore = content[content.index("ignore_packages_start")+1:content.index("ignore_packages_end")]
        
        startindices = [i for i, x in enumerate(content) if x == "target_api_rpms_start"]
        endindices = [i for i, x in enumerate(content) if x == "target_api_rpms_end"]
        for index, listed in enumerate(zip(startindices,endindices)):
            s = listed[0]
            e = listed[1]
            if content[s+1].startswith("modname: "):
                custom.append([content[s+1][9:]]+content[s+2:e])
            else:
                custom.append(["unnamed module "+str(index)]+content[s+1:e])

    return [big3,custom,ignore]

#discover if given package is in predefined modules
def isinbigthree(packname,loadedinfra):
    for key in loadedinfra:
        if packname == key:
            return "is-it"
        elif packname in loadedinfra[key]:
            return key

    return ""

#further process output of chasedeps2 and store labeling info
def pastebig3(dictionary, toignore, big3):
    #store the packages masekd by big3 to display in graph later
    maskeddeps = {}
    for key in big3:
        if key not in dictionary:
            dictionary[key] = []
        maskeddeps[key] = set()

    for key in list(dictionary):
        #if key is in big3, merge deps with big3 and remove from individual consideration
        if isinbigthree(key,big3) != "" and isinbigthree(key,big3) != "is-it":
            dictionary[isinbigthree(key,big3)]+=dictionary[key]
            del dictionary[key]
        else:

            for index, value in enumerate(list(dictionary[key])):
                #if value is in big3, replace with big3 name
                if isinbigthree(value,big3) != "" and isinbigthree(value,big3) != "is-it":
                    maskeddeps[isinbigthree(value,big3)].add(dictionary[key][index])
                    dictionary[key][index] = isinbigthree(value,big3)
                #delete values matching toignore names and self references
                if dictionary[key][index] == key or dictionary[key][index] in toignore:
                    dictionary[key][index] = ""
            #delete duplicate connections and empty values
            dictionary[key] = list(set([x for x in dictionary[key] if x]))
            #delete keys matching toignore names and empty keys, remove requested by user
            if (dictionary[key] == [] and key not in big3) or key in toignore:
                del dictionary[key]
            #below is not technically necessary due to libsolv bug
            elif "requested by user" in dictionary[key]:
                dictionary[key] = [x for x in dictionary[key] if x!="requested by user"]

    return maskeddeps

#recursively find dependencies that can be grouped into same logical module as target package
def get_loose(lookuptable, dictionary, big3):
    for key in dictionary:
        if dictionary[key] and key not in lookuptable and set(dictionary[key]).issubset(lookuptable) and key not in big3:
            lookuptable.append(key)
            get_loose(lookuptable, dictionary, big3)

#spit out grapher to new modulemd
def makemodule(packset,deps,big3):
    packs = packset[1:]
    module = modulemd.ModuleMetadata()
    module.name = packset[0]
    for rpm in packs:
        module.api.add_rpm(rpm)
    for rpm in deps:
        if isinbigthree(rpm,big3) != "" and deps[rpm]:
            module.add_requires(rpm,"f26")
        elif rpm not in packs and deps[rpm]:
            module.components.add_rpm(rpm,'FIXME: Runtime dependency for ' + ','.join(deps[rpm]))
    outputfilename = module.name + ".yaml"
    module.dump(outputfilename)
    print("modulemd output to " + os.getcwd() + "/" + outputfilename)

@click.group()
@click.option("-a", "--arch", required=True,
              help="Specify the CPU architecture.")
@click.option("-c", "--config", required=True, type=click.Path(exists=True),
              help="Path to configuration.")
@click.option("-v", "--verbose", count=True)
@click.pass_context
def cli(ctx, arch, config, verbose):
    ctx.obj["arch"] = arch
    ctx.obj["config"] = config
    ctx.obj["verbose"] = verbose
    log_conf = {}
    if verbose == 1:
        log_conf["level"] = logging.INFO
    elif verbose > 1:
        log_conf["level"] = logging.DEBUG
    logging.basicConfig(**log_conf)

@cli.command()
@click.argument("pkgnames", nargs=-1)
@click.option("--recommends/--no-recommends", default=False,
              help="Do not process optional (aka weak) dependencies.")
@click.option("--hint", multiple=True,
              help="""
Specify a package to have higher priority when more than one package could
satisfy a dependency. This option may be specified multiple times.

For example, it is recommended to use --hint=glibc-minimal-langpack.
""")
@click.option("--selfhost", is_flag=True,
              help="Look up the build dependencies as well.")
@click.pass_context
def resolve(ctx, pkgnames, recommends, hint, selfhost):
    pool = setup_pool(ctx.obj["arch"], setup_repos(ctx.obj["config"]))

    # Set up initial hints
    favorq = []
    for n in hint:
        sel = pool.select(n, solv.Selection.SELECTION_NAME)
        favorq += sel.jobs(solv.Job.SOLVER_FAVOR)
    pool.setpooljobs(favorq)

    solver = pool.Solver()
    if not recommends:
        # Ignore weak deps
        solver.set_flag(solv.Solver.SOLVER_FLAG_IGNORE_RECOMMENDED, 1)

    binary, source = solve(solver, pkgnames, selfhost=selfhost)
    for p in itertools.chain(binary, source or ()):
        print(p)

@cli.command("print-reldeps")
@click.argument("pkg")
@click.pass_context
def print_reldeps(ctx, pkg):
    pool = setup_pool(ctx.obj["arch"], setup_repos(ctx.obj["config"]))

    sel = pool.select(pkg, solv.Selection.SELECTION_NAME)
    assert not sel.isempty(), "Package can't be found"
    found = sel.solvables()
    assert len(found) == 1, "More matching solvables were found, {}".format(found)
    s = found[0]

    reldep2str = {solv.SOLVABLE_REQUIRES: "requires",
                  solv.SOLVABLE_RECOMMENDS: "recommends",
                  solv.SOLVABLE_SUGGESTS: "suggests",
                  solv.SOLVABLE_SUPPLEMENTS: "supplements",
                  solv.SOLVABLE_ENHANCES: "enhances"}
    for reltype, relstr in reldep2str.items():
        for dep in s.lookup_deparray(reltype):
            print("{}: {}".format(relstr, dep))

@cli.command()
@click.argument("inputfile", type=click.Path(exists=True))
@click.option("-g", "--generatemd", is_flag=True,
                help="Dump grapher results into modulemd file.")
@click.option("-d", "--dotfile", is_flag=True,
                help="Dump grapher results into dotfile.")
@click.pass_context
def graph(ctx, inputfile, generatemd, dotfile):
    ##reading input
    big3, custom, ignore = readgraphmakerinput(inputfile)
    big3 = onetimeload(big3)

    ##chasing deps
    print("...chasing everything at once")
    allset = []
    for packset in custom:
        allset+=packset[1:]
    #create solver and populate with rules
    hint = ()
    recommends = False
    selfhost = False
    pool = setup_pool(ctx.obj["arch"], setup_repos(ctx.obj["config"]))
    favorq = []
    for n in hint:
        sel = pool.select(n, solv.Selection.SELECTION_NAME)
        favorq += sel.jobs(solv.Job.SOLVER_FAVOR)
    pool.setpooljobs(favorq)
    solver = pool.Solver()
    if not recommends:
        solver.set_flag(solv.Solver.SOLVER_FLAG_IGNORE_RECOMMENDED, 1)
    binary, source = solve(solver, tuple(allset), selfhost=selfhost)
    output = rules_table(solver)
    #store rules in dict of values requiring keys, clean it up
    deps = chasedeps2(str(output))
    tolabel = pastebig3(deps,ignore+["fedora-release","fedora-repos"], big3)

    ##drawing graph
    #initialize inner labeling of big3
    innerlabel = {}
    for key in big3:
        innerlabel[key] = []
    #populate inner labeling
    for key in list(tolabel):
        innerlabel[key]+=list(tolabel[key])
    #generate dot structure
    dot = pydot.Dot(graph_type='digraph',simplify=True, compound=True)
    for key in deps:
        if isinbigthree(key, big3) == "is-it":
            dot.add_node(pydot.Node(key, shape="invhouse"))
    #draw edges
    for key in deps:
        for value in deps[key]:
            dot.add_edge(pydot.Edge(key,value))
    #draw modules
    for packset in custom:
        modname = packset[0]+"_module"
        packs = packset[1:]
        packsetclust = pydot.Cluster(modname, fontname="Arial Bold",label = "proposed for " + modname)

        print("...drawing " + modname)
        #draw nodes
        uptable = packs[:]
        get_loose(uptable,deps,big3)
        for key in uptable:
            if key in packs:
                packsetclust.add_node(pydot.Node(key))
            else:
                packsetclust.add_node(pydot.Node(key,shape="plaintext"))
        #draw submodules
        for item in packs:
            logicalmod = pydot.Cluster(item+"_submodule", fontname="Arial Bold",label = item+"_submodule")
            lookuptable = [item]
            get_loose(lookuptable,deps,big3)
            for key in lookuptable:
                if packsetclust.get_node(key):
                    nodename = key
                else:
                    nodename = '"' + key + '"'
                logicalmod.add_node(packsetclust.get_node(nodename)[0])
                packsetclust.del_node(packsetclust.get_node(nodename)[0])
            packsetclust.add_subgraph(logicalmod)
        dot.add_subgraph(packsetclust)
    #draw inner labels plus highlights
    for key in big3:
        if [x for x in innerlabel[key] if x in custom]:
            finallabel = key + "\n" + "\n".join([x for x in innerlabel[key] if x in custom])
            dot.add_node(pydot.Node(key, shape = "box",label=finallabel,color="red"))

    ##outputting file
    outputfilename = 'graph.svg'
    dot.write_svg(outputfilename)
    print("success! graph output to " + os.getcwd() + "/" + outputfilename)

    if dotfile:
        outputfilename = 'dot.dot'
        dot.write(outputfilename)
        print("dotfile output to " + os.getcwd() + "/" + outputfilename)

    ##optional output modulemd
    if generatemd:
        for packset in custom:
            #create solver and populate with rules
            hint = ()
            recommends = False
            selfhost = False
            pool = setup_pool(ctx.obj["arch"], setup_repos(ctx.obj["config"]))
            favorq = []
            for n in hint:
                sel = pool.select(n, solv.Selection.SELECTION_NAME)
                favorq += sel.jobs(solv.Job.SOLVER_FAVOR)
            pool.setpooljobs(favorq)
            solver = pool.Solver()
            if not recommends:
                solver.set_flag(solv.Solver.SOLVER_FLAG_IGNORE_RECOMMENDED, 1)
            binary, source = solve(solver, tuple(packset[1:]), selfhost=selfhost)
            output = rules_table(solver)
            #store rules in dict of values requiring keys, clean it up
            deps = chasedeps2(str(output))
            tolabel = pastebig3(deps,ignore+["fedora-release","fedora-repos"], big3)
            makemodule(packset,deps,big3)

if __name__ == "__main__":
    cli(obj={})
